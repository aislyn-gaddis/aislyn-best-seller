{
  "hash": "7eb288e36bb3b7bc7ab94559a8ef95fa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Cleaning\"\n---\n\n\n\n## Goals of the notebook\n\nIn this notebook, I'm going to \n\n- import the Post45 Data Collective's NYT bestseller data set\n- web scrape the NYT Hardcover Fiction List\n- web scrape the NYT Combined Print and E-Book Fiction List\n- clean the data\n- join the two hardcover fiction data sets \n- save the data\n\nI'll load any libraries I might need.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n\n\n:::\n:::\n\n\n\n## Main Data Import\n\nI'll import the years I already have. I downloaded this from the [Post45 Data Collective](https://data.post45.org/new-york-times-hardcover-fiction-bestsellers-1931-2020-curatorial-statement/) which is run by Emory University. It's every week of the New York Times Bestsellers list going back to when it started in 1931 up to December 2020.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers <- read_tsv(\"data-raw/nyt_full.tsv\") |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 60386 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): title, author\ndbl  (3): year, rank, title_id\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 60,386\nColumns: 6\n$ year     <dbl> 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1…\n$ week     <date> 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, …\n$ rank     <dbl> 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2…\n$ title_id <dbl> 6477, 1808, 5304, 4038, 3946, 2878, 7031, 413, 859, 4235, 287…\n$ title    <chr> \"THE TEN COMMANDMENTS\", \"FINCHE'S FORTUNE\", \"THE GOOD EARTH\",…\n$ author   <chr> \"Warwick Deeping\", \"Mazo de la Roche\", \"Pearl S. Buck\", \"Will…\n```\n\n\n:::\n:::\n\n\n\n## Hardcover Fiction Web Scrape\n\nNow, I'm going to get the missing years of the hardcover fiction data by web scraping directly from the NYT's website. You might might need to run the code multiple times before it will successfully complete. I also recommend you comment the code after completion since it takes a bit to run.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_date <- as.Date(\"2020-12-13\")\nend_date <- as.Date(\"2024-10-06\")\n\ndates <- seq(start_date, end_date, by = \"week\")\n\ndfs <- map_dfr(dates, function(date) {\n  url <- sprintf(\"https://www.nytimes.com/books/best-sellers/%s/hardcover-fiction/\", format(date, \"%Y/%m/%d\"))\n  \n  page <- read_html(url)\n  \n  titles <- page %>%\n    html_nodes(\".css-5pe77f\") %>%\n    html_text()\n  \n  authors <- page %>%\n    html_nodes(\".css-hjukut\") %>%\n    html_text()\n  \n  ranks <- seq_along(titles)\n  \n  data.frame(title = titles, author = authors, rank = ranks, date = date)\n})\n\ndfs |> write_rds(\"data-raw/bestsellers-web-scrape.rds\")\n```\n:::\n\n\n\nI'll save the new file into a new object and glimpse it. I also saved the data into a csv just for the purpose of importing it to google sheets later.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_two <- read_rds(\"data-raw/bestsellers-web-scrape.rds\") # creating a new object with a file \n\nbestsellers_two |> write_csv(\"data-raw/bestsellers-web-scrape.csv\") # saving the object into a csv to be able to upload it to google sheets\n\nbestsellers_two |> glimpse() # glimpsing the data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,000\nColumns: 4\n$ title  <chr> \"READY PLAYER TWO\", \"DEADLY CROSS\", \"THE RETURN\", \"A TIME FOR M…\n$ author <chr> \"by Ernest Cline\", \"by James Patterson\", \"by Nicholas Sparks\", …\n$ rank   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, …\n$ date   <date> 2020-12-13, 2020-12-13, 2020-12-13, 2020-12-13, 2020-12-13, 20…\n```\n\n\n:::\n:::\n\n\n\n## Combined Print and E-book Web Scrape\n\nNow I'll do a similar web scrape to get the combined print and ebook list. This is going through over a decade of data, so you may have to try running it a few times before it successfully completes. I also recommend commenting it after you finish running it. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_date <- as.Date(\"2011-02-13\")\nend_date <- as.Date(\"2024-10-06\")\n\ndates <- seq(start_date, end_date, by = \"week\")\n\ndfs <- map_dfr(dates, function(date) {\n  url <- sprintf(\"https://www.nytimes.com/books/best-sellers/%s/combined-print-and-e-book-fiction/\", format(date, \"%Y/%m/%d\"))\n  \n  page <- tryCatch(read_html(url), error = function(e) NULL)\n  \n  if (!is.null(page)) {\n    titles <- page %>%\n    html_nodes(\".css-5pe77f\") %>%\n    html_text()\n\n    authors <- page %>%\n    html_nodes(\".css-hjukut\") %>%\n    html_text()\n    \n    ranks <- seq_along(titles)\n    \n    data.frame(title = titles, author = authors, rank = ranks, date = date)\n  } else {\n    NULL\n  }\n})\n\ndfs |> write_rds(\"data-raw/bestsellers-combined-web-scrape.rds\")\n\nbestsellers_combined <- read_rds(\"data-raw/bestsellers-combined-web-scrape.rds\") \n\nbestsellers_combined |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,695\nColumns: 4\n$ title  <chr> \"TICK TOCK\", \"THE GIRL WITH THE DRAGON TATTOO\", \"THE GIRL WHO P…\n$ author <chr> \"by James Patterson and Michael Ledwidge\", \"by Stieg Larsson\", …\n$ rank   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, …\n$ date   <date> 2011-02-13, 2011-02-13, 2011-02-13, 2011-02-13, 2011-02-13, 20…\n```\n\n\n:::\n:::\n\n\n\n## Cleaning\n\nFirst, I'll clean the main data file from the Post45 data collective. The only thing I need to do is remove the title_id column since I won't be using it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_clean <- bestsellers |> # saving this chunk into a new object and starting with the data\n  select(-title_id) |> # removing the title_id column\n  glimpse() # glimpsing the data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 60,386\nColumns: 5\n$ year   <dbl> 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 193…\n$ week   <date> 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 19…\n$ rank   <dbl> 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, …\n$ title  <chr> \"THE TEN COMMANDMENTS\", \"FINCHE'S FORTUNE\", \"THE GOOD EARTH\", \"…\n$ author <chr> \"Warwick Deeping\", \"Mazo de la Roche\", \"Pearl S. Buck\", \"Willa …\n```\n\n\n:::\n:::\n\n\n\nNow, I'll clean the web scrape to make it match the first dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_two_clean <- bestsellers_two |> # saving this chunk into a new object and starting with the data\n  mutate(year = year(date), # making a year column\n         week = date, # making a new date column called \"week\" to match the first dataset\n         author = str_remove_all(author, \"by \"), # removing the \"by \" from the author column\n         rank = as.numeric(rank)) |> # changing the rank column from int to dbl\n  select(year,\n         week,\n         rank,\n         title,\n         author) |> # putting the columns in the same order as the first dataset\n  glimpse() # glimpsing the data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,000\nColumns: 5\n$ year   <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 202…\n$ week   <date> 2020-12-13, 2020-12-13, 2020-12-13, 2020-12-13, 2020-12-13, 20…\n$ rank   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, …\n$ title  <chr> \"READY PLAYER TWO\", \"DEADLY CROSS\", \"THE RETURN\", \"A TIME FOR M…\n$ author <chr> \"Ernest Cline\", \"James Patterson\", \"Nicholas Sparks\", \"John Gri…\n```\n\n\n:::\n:::\n\n\n\n\nI'll do the same for the combined list. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_combined_clean <- bestsellers_combined |> # saving this chunk into a new object and starting with the data\n  mutate(year = year(date), # making a year column\n         week = date, # making a new date column called \"week\" to match the first dataset\n         author = str_remove_all(author, \"by \"), # removing the \"by \" from the author column\n         rank = as.numeric(rank)) |> # changing the rank column from int to dbl\n  select(year,\n         week,\n         rank,\n         title,\n         author) |> # putting the columns in the same order as the first dataset\n  glimpse() # glimpsing the data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,695\nColumns: 5\n$ year   <dbl> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 201…\n$ week   <date> 2011-02-13, 2011-02-13, 2011-02-13, 2011-02-13, 2011-02-13, 20…\n$ rank   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, …\n$ title  <chr> \"TICK TOCK\", \"THE GIRL WITH THE DRAGON TATTOO\", \"THE GIRL WHO P…\n$ author <chr> \"James Patterson and Michael Ledwidge\", \"Stieg Larsson\", \"Stieg…\n```\n\n\n:::\n:::\n\n\n\n## Combining the data\n\nNow that both of the hardcover fiction datasets have the same columns, I'll combine them.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_full <- bestsellers_clean |> # saving this chunk into a new object and starting with the data\n  bind_rows(bestsellers_two_clean) |> # binding the two datasets together\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 63,386\nColumns: 5\n$ year   <dbl> 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 193…\n$ week   <date> 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 19…\n$ rank   <dbl> 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, …\n$ title  <chr> \"THE TEN COMMANDMENTS\", \"FINCHE'S FORTUNE\", \"THE GOOD EARTH\", \"…\n$ author <chr> \"Warwick Deeping\", \"Mazo de la Roche\", \"Pearl S. Buck\", \"Willa …\n```\n\n\n:::\n:::\n\n\n\n## Fixing non-split columns\n\nThere are some columns where the title and author aren't split properly.\n\nI'll start by getting rid of those columns from the main data set, then creating a new object with just those columns to fix them.\n\nThen, I separate the title and author into new columns using the , in between them. I'll also remove the by and the publisher information\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_no_na <- bestsellers_full |> # saving the data into a new object\n  filter(!is.na(author)) # removing columns where author is na\n\nbestsellers_na <- bestsellers_full |> # saving the data into a new object\n  filter(is.na(author)) # only including columns where author is na\n\nbestsellers_na_clean <- bestsellers_na |> # saving the data into a new object\n  separate(title, sep = \",\", into = c(\"title\", \"author\")) |> # separate the title column into two columns named title and author based on the comma\n  mutate(author = str_remove_all(author, \"by \")) |> # removing the by\n  mutate(author = str_remove_all(author, \"\\\\(.*\")) |> # removing the publisher information using the ()\n  mutate(author = str_sub(author, end = -3))# removing the period and spaces by removing the last 3 characters of the column\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Expected 2 pieces. Additional pieces discarded in 7 rows [1, 2, 3, 4,\n5, 6, 7].\n```\n\n\n:::\n\n```{.r .cell-code}\nbestsellers_na_clean\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"year\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"week\"],\"name\":[2],\"type\":[\"date\"],\"align\":[\"right\"]},{\"label\":[\"rank\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"title\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"author\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1932\",\"2\":\"1932-11-14\",\"3\":\"2\",\"4\":\"MUTINY ON THE BOUNTY\",\"5\":\"Charles Nordhoff and James N. Hall\"},{\"1\":\"1934\",\"2\":\"1934-08-13\",\"3\":\"5\",\"4\":\"HOLY DEADLOCK\",\"5\":\"A. P. Herbert\"},{\"1\":\"1934\",\"2\":\"1934-08-20\",\"3\":\"4\",\"4\":\"HOLY DEADLOCK\",\"5\":\"A. P. Herbert\"},{\"1\":\"1934\",\"2\":\"1934-08-27\",\"3\":\"4\",\"4\":\"HOLY DEADLOCK\",\"5\":\"A. P. Herbert\"},{\"1\":\"1934\",\"2\":\"1934-08-27\",\"3\":\"5\",\"4\":\"EAST AND WEST\",\"5\":\"Somerset Maugham\"},{\"1\":\"1934\",\"2\":\"1934-09-03\",\"3\":\"8\",\"4\":\"HOLY DEADLOCK\",\"5\":\"A. P. Herbert\"},{\"1\":\"1935\",\"2\":\"1935-12-16\",\"3\":\"8\",\"4\":\"MUTINY ON THE BOUNTY\",\"5\":\"Charles Nordhoff and James N. Hall\"},{\"1\":\"1937\",\"2\":\"1937-06-21\",\"3\":\"7\",\"4\":\"A CITY OF BELLS\",\"5\":\"Elizabeth Goudge\"},{\"1\":\"1937\",\"2\":\"1937-07-05\",\"3\":\"7\",\"4\":\"A CITY OF BELLS\",\"5\":\"Elizabeth Goudge\"},{\"1\":\"1937\",\"2\":\"1937-09-27\",\"3\":\"6\",\"4\":\"A CITY OF BELLS\",\"5\":\"Elizabeth Goudge\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## Joining the data again\n\nNow that I've fixed the rows that didn't separate properly, I'll join everything back together.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_full_clean <- bestsellers_no_na |> \n  bind_rows(bestsellers_na_clean) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 63,386\nColumns: 5\n$ year   <dbl> 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 193…\n$ week   <date> 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 1931-10-12, 19…\n$ rank   <dbl> 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, …\n$ title  <chr> \"THE TEN COMMANDMENTS\", \"FINCHE'S FORTUNE\", \"THE GOOD EARTH\", \"…\n$ author <chr> \"Warwick Deeping\", \"Mazo de la Roche\", \"Pearl S. Buck\", \"Willa …\n```\n\n\n:::\n:::\n\n\nIt looks like everything parsed correctly in the combined list data, so I don't need to worry about that.\n\n## Exporting the data\n\nI'll export the data to my computer as an rds and as a csv\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestsellers_full_clean |> write_rds(\"data-processed/bestsellers-full.rds\")\nbestsellers_full_clean |> write_csv(\"data-processed/bestsellers-full.csv\")\n\nbestsellers_combined_clean |> write_rds(\"data-processed/bestsellers-combined.rds\")\nbestsellers_combined_clean |> write_csv(\"data-processed/bestsellers-combined.csv\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}